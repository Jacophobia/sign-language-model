{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [
    {
     "data": {
      "text/plain": "         x1        y1            z1        x2        y2        z2        x3  \\\n0  0.284234  0.599662  1.510391e-09  0.352661  0.574240 -0.018644  0.401655   \n1  0.285704  0.594281 -4.252064e-09  0.351391  0.575788 -0.023778  0.397199   \n2  0.282022  0.592181 -4.262098e-09  0.348222  0.574958 -0.024807  0.395815   \n3  0.279870  0.594496 -3.989998e-09  0.345210  0.575632 -0.021771  0.391491   \n4  0.275030  0.592625 -4.403823e-09  0.340713  0.573089 -0.021498  0.387466   \n\n         y3        z3        x4  ...       x19       y19       z19       x20  \\\n0  0.505305 -0.031250  0.417682  ...  0.264750  0.411588 -0.051163  0.270928   \n1  0.503982 -0.038397  0.411797  ...  0.260840  0.409029 -0.054180  0.269301   \n2  0.503679 -0.039821  0.411231  ...  0.257454  0.408016 -0.052401  0.265423   \n3  0.502638 -0.035953  0.406466  ...  0.254278  0.407087 -0.054067  0.262207   \n4  0.500672 -0.035819  0.402215  ...  0.248197  0.406337 -0.052727  0.256451   \n\n        y20       z20       x21       y21       z21  symbol  \n0  0.466022 -0.052875  0.276105  0.505524 -0.052011       0  \n1  0.466309 -0.058145  0.274917  0.504831 -0.057939       0  \n2  0.464812 -0.056430  0.270709  0.503389 -0.056365       0  \n3  0.463662 -0.057304  0.266132  0.502502 -0.056816       0  \n4  0.461449 -0.056080  0.261073  0.500557 -0.055821       0  \n\n[5 rows x 64 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x1</th>\n      <th>y1</th>\n      <th>z1</th>\n      <th>x2</th>\n      <th>y2</th>\n      <th>z2</th>\n      <th>x3</th>\n      <th>y3</th>\n      <th>z3</th>\n      <th>x4</th>\n      <th>...</th>\n      <th>x19</th>\n      <th>y19</th>\n      <th>z19</th>\n      <th>x20</th>\n      <th>y20</th>\n      <th>z20</th>\n      <th>x21</th>\n      <th>y21</th>\n      <th>z21</th>\n      <th>symbol</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.284234</td>\n      <td>0.599662</td>\n      <td>1.510391e-09</td>\n      <td>0.352661</td>\n      <td>0.574240</td>\n      <td>-0.018644</td>\n      <td>0.401655</td>\n      <td>0.505305</td>\n      <td>-0.031250</td>\n      <td>0.417682</td>\n      <td>...</td>\n      <td>0.264750</td>\n      <td>0.411588</td>\n      <td>-0.051163</td>\n      <td>0.270928</td>\n      <td>0.466022</td>\n      <td>-0.052875</td>\n      <td>0.276105</td>\n      <td>0.505524</td>\n      <td>-0.052011</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.285704</td>\n      <td>0.594281</td>\n      <td>-4.252064e-09</td>\n      <td>0.351391</td>\n      <td>0.575788</td>\n      <td>-0.023778</td>\n      <td>0.397199</td>\n      <td>0.503982</td>\n      <td>-0.038397</td>\n      <td>0.411797</td>\n      <td>...</td>\n      <td>0.260840</td>\n      <td>0.409029</td>\n      <td>-0.054180</td>\n      <td>0.269301</td>\n      <td>0.466309</td>\n      <td>-0.058145</td>\n      <td>0.274917</td>\n      <td>0.504831</td>\n      <td>-0.057939</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.282022</td>\n      <td>0.592181</td>\n      <td>-4.262098e-09</td>\n      <td>0.348222</td>\n      <td>0.574958</td>\n      <td>-0.024807</td>\n      <td>0.395815</td>\n      <td>0.503679</td>\n      <td>-0.039821</td>\n      <td>0.411231</td>\n      <td>...</td>\n      <td>0.257454</td>\n      <td>0.408016</td>\n      <td>-0.052401</td>\n      <td>0.265423</td>\n      <td>0.464812</td>\n      <td>-0.056430</td>\n      <td>0.270709</td>\n      <td>0.503389</td>\n      <td>-0.056365</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.279870</td>\n      <td>0.594496</td>\n      <td>-3.989998e-09</td>\n      <td>0.345210</td>\n      <td>0.575632</td>\n      <td>-0.021771</td>\n      <td>0.391491</td>\n      <td>0.502638</td>\n      <td>-0.035953</td>\n      <td>0.406466</td>\n      <td>...</td>\n      <td>0.254278</td>\n      <td>0.407087</td>\n      <td>-0.054067</td>\n      <td>0.262207</td>\n      <td>0.463662</td>\n      <td>-0.057304</td>\n      <td>0.266132</td>\n      <td>0.502502</td>\n      <td>-0.056816</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.275030</td>\n      <td>0.592625</td>\n      <td>-4.403823e-09</td>\n      <td>0.340713</td>\n      <td>0.573089</td>\n      <td>-0.021498</td>\n      <td>0.387466</td>\n      <td>0.500672</td>\n      <td>-0.035819</td>\n      <td>0.402215</td>\n      <td>...</td>\n      <td>0.248197</td>\n      <td>0.406337</td>\n      <td>-0.052727</td>\n      <td>0.256451</td>\n      <td>0.461449</td>\n      <td>-0.056080</td>\n      <td>0.261073</td>\n      <td>0.500557</td>\n      <td>-0.055821</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 64 columns</p>\n</div>"
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data.\n",
    "# The data is stored in a bunch of csv files\n",
    "# for example data for the letter A is stored in the file \"training_data_0/A.csv\"\n",
    "# we will load all the data into a pandas dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "num_to_letter = dict()\n",
    "letter_to_num = dict()\n",
    "\n",
    "# index and loop through the letters\n",
    "for i, letter in enumerate(\"ABCDEFGHIKLMNOPQRSTUVWXY\"):\n",
    "    # read the csv file\n",
    "    df_letter = pd.read_csv(f\"training_data_0/{letter}.csv\")\n",
    "    # drop the \"'symbol'\" column\n",
    "    df_letter = df_letter.drop(columns=[\"'symbol'\"])\n",
    "    # add a column to the dataframe with the letter\n",
    "    df_letter[\"symbol\"] = i\n",
    "    # add the letter to the letter maps\n",
    "    num_to_letter[i] = letter\n",
    "    letter_to_num[letter] = i\n",
    "    # add the dataframe to the main dataframe\n",
    "    df = pd.concat([df, df_letter])\n",
    "\n",
    "# rename the \"'symbol'\" column to \"symbol\"\n",
    "df = df.rename(columns={\"'symbol'\": \"symbol\"})\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [
    {
     "data": {
      "text/plain": "4     375\n2     313\n17    298\n11    283\n14    260\n20    257\n12    256\n19    252\n15    248\n10    247\n22    246\n9     245\n21    244\n3     243\n7     241\n18    238\n6     237\n5     237\n1     234\n23    233\n16    233\n13    233\n8     233\n0     225\nName: symbol, dtype: int64"
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"symbol\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [
    {
     "data": {
      "text/plain": "                x1           y1            z1           x2           y2  \\\ncount  6111.000000  6111.000000  6.111000e+03  6111.000000  6111.000000   \nmean      0.440433     0.622750  5.929709e-10     0.486052     0.603479   \nstd       0.191912     0.181983  5.487365e-09     0.196405     0.182482   \nmin       0.090501     0.141086 -1.969110e-08     0.109391     0.128427   \n25%       0.289168     0.476999 -2.008002e-09     0.327600     0.453367   \n50%       0.390545     0.595534  1.151827e-09     0.441132     0.579819   \n75%       0.626650     0.792995  3.734280e-09     0.678022     0.777037   \nmax       0.836572     1.025575  1.916064e-08     0.898871     0.997497   \n\n                z2           x3           y3           z3           x4  ...  \\\ncount  6111.000000  6111.000000  6111.000000  6111.000000  6111.000000  ...   \nmean     -0.018906     0.518731     0.554779    -0.037185     0.521704  ...   \nstd       0.022592     0.204327     0.187479     0.033304     0.217539  ...   \nmin      -0.107338     0.103584     0.105467    -0.156331     0.059861  ...   \n25%      -0.031281     0.355535     0.392496    -0.053778     0.344039  ...   \n50%      -0.019073     0.478165     0.540500    -0.036178     0.477882  ...   \n75%      -0.007548     0.720200     0.731449    -0.021938     0.738128  ...   \nmax       0.082677     0.945384     0.950460     0.117195     0.976453  ...   \n\n               x19          y19          z19          x20          y20  \\\ncount  6111.000000  6111.000000  6111.000000  6111.000000  6111.000000   \nmean      0.430614     0.464734    -0.072318     0.439989     0.486142   \nstd       0.220695     0.194340     0.026841     0.221597     0.198481   \nmin       0.023418     0.080981    -0.191882     0.025751     0.072168   \n25%       0.251514     0.305164    -0.088480     0.258871     0.333067   \n50%       0.379583     0.441283    -0.073844     0.391482     0.463063   \n75%       0.647629     0.627818    -0.056358     0.659670     0.630992   \nmax       0.916674     0.986162     0.053344     0.903740     1.020965   \n\n               z20          x21          y21          z21       symbol  \ncount  6111.000000  6111.000000  6111.000000  6111.000000  6111.000000  \nmean     -0.076378     0.445179     0.497874    -0.077505    11.356079  \nstd       0.026343     0.219653     0.201887     0.027505     6.902894  \nmin      -0.199254     0.031057     0.041557    -0.197314     0.000000  \n25%      -0.090959     0.265973     0.349207    -0.092341     5.000000  \n50%      -0.075681     0.394272     0.477647    -0.075551    11.000000  \n75%      -0.061090     0.659340     0.644602    -0.061332    17.000000  \nmax       0.040676     0.890713     1.031843     0.034286    23.000000  \n\n[8 rows x 64 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x1</th>\n      <th>y1</th>\n      <th>z1</th>\n      <th>x2</th>\n      <th>y2</th>\n      <th>z2</th>\n      <th>x3</th>\n      <th>y3</th>\n      <th>z3</th>\n      <th>x4</th>\n      <th>...</th>\n      <th>x19</th>\n      <th>y19</th>\n      <th>z19</th>\n      <th>x20</th>\n      <th>y20</th>\n      <th>z20</th>\n      <th>x21</th>\n      <th>y21</th>\n      <th>z21</th>\n      <th>symbol</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6.111000e+03</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>...</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.440433</td>\n      <td>0.622750</td>\n      <td>5.929709e-10</td>\n      <td>0.486052</td>\n      <td>0.603479</td>\n      <td>-0.018906</td>\n      <td>0.518731</td>\n      <td>0.554779</td>\n      <td>-0.037185</td>\n      <td>0.521704</td>\n      <td>...</td>\n      <td>0.430614</td>\n      <td>0.464734</td>\n      <td>-0.072318</td>\n      <td>0.439989</td>\n      <td>0.486142</td>\n      <td>-0.076378</td>\n      <td>0.445179</td>\n      <td>0.497874</td>\n      <td>-0.077505</td>\n      <td>11.356079</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.191912</td>\n      <td>0.181983</td>\n      <td>5.487365e-09</td>\n      <td>0.196405</td>\n      <td>0.182482</td>\n      <td>0.022592</td>\n      <td>0.204327</td>\n      <td>0.187479</td>\n      <td>0.033304</td>\n      <td>0.217539</td>\n      <td>...</td>\n      <td>0.220695</td>\n      <td>0.194340</td>\n      <td>0.026841</td>\n      <td>0.221597</td>\n      <td>0.198481</td>\n      <td>0.026343</td>\n      <td>0.219653</td>\n      <td>0.201887</td>\n      <td>0.027505</td>\n      <td>6.902894</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.090501</td>\n      <td>0.141086</td>\n      <td>-1.969110e-08</td>\n      <td>0.109391</td>\n      <td>0.128427</td>\n      <td>-0.107338</td>\n      <td>0.103584</td>\n      <td>0.105467</td>\n      <td>-0.156331</td>\n      <td>0.059861</td>\n      <td>...</td>\n      <td>0.023418</td>\n      <td>0.080981</td>\n      <td>-0.191882</td>\n      <td>0.025751</td>\n      <td>0.072168</td>\n      <td>-0.199254</td>\n      <td>0.031057</td>\n      <td>0.041557</td>\n      <td>-0.197314</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.289168</td>\n      <td>0.476999</td>\n      <td>-2.008002e-09</td>\n      <td>0.327600</td>\n      <td>0.453367</td>\n      <td>-0.031281</td>\n      <td>0.355535</td>\n      <td>0.392496</td>\n      <td>-0.053778</td>\n      <td>0.344039</td>\n      <td>...</td>\n      <td>0.251514</td>\n      <td>0.305164</td>\n      <td>-0.088480</td>\n      <td>0.258871</td>\n      <td>0.333067</td>\n      <td>-0.090959</td>\n      <td>0.265973</td>\n      <td>0.349207</td>\n      <td>-0.092341</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.390545</td>\n      <td>0.595534</td>\n      <td>1.151827e-09</td>\n      <td>0.441132</td>\n      <td>0.579819</td>\n      <td>-0.019073</td>\n      <td>0.478165</td>\n      <td>0.540500</td>\n      <td>-0.036178</td>\n      <td>0.477882</td>\n      <td>...</td>\n      <td>0.379583</td>\n      <td>0.441283</td>\n      <td>-0.073844</td>\n      <td>0.391482</td>\n      <td>0.463063</td>\n      <td>-0.075681</td>\n      <td>0.394272</td>\n      <td>0.477647</td>\n      <td>-0.075551</td>\n      <td>11.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.626650</td>\n      <td>0.792995</td>\n      <td>3.734280e-09</td>\n      <td>0.678022</td>\n      <td>0.777037</td>\n      <td>-0.007548</td>\n      <td>0.720200</td>\n      <td>0.731449</td>\n      <td>-0.021938</td>\n      <td>0.738128</td>\n      <td>...</td>\n      <td>0.647629</td>\n      <td>0.627818</td>\n      <td>-0.056358</td>\n      <td>0.659670</td>\n      <td>0.630992</td>\n      <td>-0.061090</td>\n      <td>0.659340</td>\n      <td>0.644602</td>\n      <td>-0.061332</td>\n      <td>17.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.836572</td>\n      <td>1.025575</td>\n      <td>1.916064e-08</td>\n      <td>0.898871</td>\n      <td>0.997497</td>\n      <td>0.082677</td>\n      <td>0.945384</td>\n      <td>0.950460</td>\n      <td>0.117195</td>\n      <td>0.976453</td>\n      <td>...</td>\n      <td>0.916674</td>\n      <td>0.986162</td>\n      <td>0.053344</td>\n      <td>0.903740</td>\n      <td>1.020965</td>\n      <td>0.040676</td>\n      <td>0.890713</td>\n      <td>1.031843</td>\n      <td>0.034286</td>\n      <td>23.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 64 columns</p>\n</div>"
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [
    {
     "data": {
      "text/plain": "x1        float64\ny1        float64\nz1        float64\nx2        float64\ny2        float64\n           ...   \nz20       float64\nx21       float64\ny21       float64\nz21       float64\nsymbol      int64\nLength: 64, dtype: object"
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.84234434e-01  5.99662066e-01  1.51039137e-09  3.52660865e-01\n",
      "  5.74240446e-01 -1.86437536e-02  4.01654959e-01  5.05305409e-01\n",
      " -3.12500969e-02  4.17681813e-01  4.36548412e-01 -4.56334427e-02\n",
      "  4.22503233e-01  3.77860457e-01 -5.84865883e-02  3.74746442e-01\n",
      "  4.36910540e-01 -8.12254008e-03  3.80444705e-01  3.89682174e-01\n",
      " -2.00392306e-02  3.67883712e-01  4.49001282e-01 -2.07441952e-02\n",
      "  3.63591522e-01  4.97271508e-01 -2.23034602e-02  3.35153967e-01\n",
      "  4.29939628e-01 -1.52731035e-02  3.40880424e-01  3.88413638e-01\n",
      " -4.07350659e-02  3.35393161e-01  4.62964505e-01 -5.23742251e-02\n",
      "  3.34779263e-01  5.15209615e-01 -5.54651171e-02  2.94720203e-01\n",
      "  4.33333546e-01 -2.50072163e-02  3.02414715e-01  3.95723999e-01\n",
      " -5.02379835e-02  3.02014083e-01  4.72031504e-01 -5.67812920e-02\n",
      "  3.04903924e-01  5.23163736e-01 -5.66067770e-02  2.54041135e-01\n",
      "  4.42956179e-01 -3.69402952e-02  2.64749527e-01  4.11588371e-01\n",
      " -5.11631295e-02  2.70927608e-01  4.66022164e-01 -5.28754666e-02\n",
      "  2.76105493e-01  5.05524457e-01 -5.20110615e-02  0.00000000e+00]\n",
      "[ 2.84234434e-01  5.99662066e-01  1.51039137e-09  3.52660865e-01\n",
      "  5.74240446e-01 -1.86437536e-02  4.01654959e-01  5.05305409e-01\n",
      " -3.12500969e-02  4.17681813e-01  4.36548412e-01 -4.56334427e-02\n",
      "  4.22503233e-01  3.77860457e-01 -5.84865883e-02  3.74746442e-01\n",
      "  4.36910540e-01 -8.12254008e-03  3.80444705e-01  3.89682174e-01\n",
      " -2.00392306e-02  3.67883712e-01  4.49001282e-01 -2.07441952e-02\n",
      "  3.63591522e-01  4.97271508e-01 -2.23034602e-02  3.35153967e-01\n",
      "  4.29939628e-01 -1.52731035e-02  3.40880424e-01  3.88413638e-01\n",
      " -4.07350659e-02  3.35393161e-01  4.62964505e-01 -5.23742251e-02\n",
      "  3.34779263e-01  5.15209615e-01 -5.54651171e-02  2.94720203e-01\n",
      "  4.33333546e-01 -2.50072163e-02  3.02414715e-01  3.95723999e-01\n",
      " -5.02379835e-02  3.02014083e-01  4.72031504e-01 -5.67812920e-02\n",
      "  3.04903924e-01  5.23163736e-01 -5.66067770e-02  2.54041135e-01\n",
      "  4.42956179e-01 -3.69402952e-02  2.64749527e-01  4.11588371e-01\n",
      " -5.11631295e-02  2.70927608e-01  4.66022164e-01 -5.28754666e-02\n",
      "  2.76105493e-01  5.05524457e-01 -5.20110615e-02]\n"
     ]
    }
   ],
   "source": [
    "# we need to modify the data. Each row represents the xyz coordinates of several points on a hand\n",
    "# we will convert this into a series of distances between the points and the center of the hand\n",
    "normalized_df = pd.DataFrame()\n",
    "# get the center of the hand\n",
    "first = True\n",
    "for row in df.iterrows():\n",
    "    # get the xyz coordinates of the points\n",
    "    if first:\n",
    "        print(row[1][:].values)\n",
    "        print(row[1][:-1].values)\n",
    "        first = False\n",
    "    points = row[1][:-1].values.reshape(-1, 3)\n",
    "    # get the center of the hand\n",
    "    center = np.mean(points, axis=0)\n",
    "    # get the distances between the points and the center\n",
    "    distances = np.linalg.norm(points - center, axis=1)\n",
    "    # add the distances to the normalized dataframe\n",
    "    normalized_df = pd.concat([normalized_df, pd.DataFrame(distances).T])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [
    {
     "data": {
      "text/plain": "                0            1            2            3            4   \\\ncount  6111.000000  6111.000000  6111.000000  6111.000000  6111.000000   \nmean      0.168880     0.135518     0.092419     0.070245     0.078422   \nstd       0.039238     0.037373     0.034306     0.030727     0.031394   \nmin       0.038313     0.030872     0.022458     0.001777     0.007949   \n25%       0.140355     0.108687     0.065736     0.045820     0.056440   \n50%       0.163896     0.128998     0.084116     0.069041     0.074566   \n75%       0.194633     0.158656     0.115661     0.088772     0.096883   \nmax       0.271586     0.242121     0.192109     0.169416     0.160228   \n\n                5            6            7            8            9   ...  \\\ncount  6111.000000  6111.000000  6111.000000  6111.000000  6111.000000  ...   \nmean      0.062430     0.080998     0.092487     0.110193     0.049627  ...   \nstd       0.013824     0.021424     0.038436     0.054275     0.019953  ...   \nmin       0.002870     0.011499     0.007262     0.002656     0.009778  ...   \n25%       0.052694     0.067097     0.058526     0.059297     0.035693  ...   \n50%       0.060650     0.080629     0.088747     0.107295     0.043712  ...   \n75%       0.070891     0.092415     0.123537     0.156276     0.057363  ...   \nmax       0.114997     0.146094     0.210620     0.263544     0.119883  ...   \n\n                11           12           13           14           15  \\\ncount  6111.000000  6111.000000  6111.000000  6111.000000  6111.000000   \nmean      0.069930     0.089970     0.054527     0.052702     0.055431   \nstd       0.043893     0.063577     0.018517     0.020980     0.028128   \nmin       0.006704     0.004441     0.008384     0.005722     0.003783   \n25%       0.032528     0.034391     0.040797     0.037143     0.034840   \n50%       0.054058     0.068965     0.050821     0.046897     0.045515   \n75%       0.111149     0.157310     0.064120     0.067974     0.070391   \nmax       0.166165     0.220927     0.115243     0.118714     0.149044   \n\n                16           17           18           19           20  \ncount  6111.000000  6111.000000  6111.000000  6111.000000  6111.000000  \nmean      0.074259     0.076923     0.068345     0.074658     0.087115  \nstd       0.041497     0.015087     0.019747     0.027200     0.036576  \nmin       0.002551     0.007935     0.010838     0.014071     0.013151  \n25%       0.046042     0.066589     0.054724     0.057067     0.061895  \n50%       0.061525     0.078056     0.062134     0.069940     0.076313  \n75%       0.088147     0.088172     0.081652     0.083545     0.104001  \nmax       0.203736     0.121115     0.141722     0.183354     0.221810  \n\n[8 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>...</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n      <td>6111.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.168880</td>\n      <td>0.135518</td>\n      <td>0.092419</td>\n      <td>0.070245</td>\n      <td>0.078422</td>\n      <td>0.062430</td>\n      <td>0.080998</td>\n      <td>0.092487</td>\n      <td>0.110193</td>\n      <td>0.049627</td>\n      <td>...</td>\n      <td>0.069930</td>\n      <td>0.089970</td>\n      <td>0.054527</td>\n      <td>0.052702</td>\n      <td>0.055431</td>\n      <td>0.074259</td>\n      <td>0.076923</td>\n      <td>0.068345</td>\n      <td>0.074658</td>\n      <td>0.087115</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.039238</td>\n      <td>0.037373</td>\n      <td>0.034306</td>\n      <td>0.030727</td>\n      <td>0.031394</td>\n      <td>0.013824</td>\n      <td>0.021424</td>\n      <td>0.038436</td>\n      <td>0.054275</td>\n      <td>0.019953</td>\n      <td>...</td>\n      <td>0.043893</td>\n      <td>0.063577</td>\n      <td>0.018517</td>\n      <td>0.020980</td>\n      <td>0.028128</td>\n      <td>0.041497</td>\n      <td>0.015087</td>\n      <td>0.019747</td>\n      <td>0.027200</td>\n      <td>0.036576</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.038313</td>\n      <td>0.030872</td>\n      <td>0.022458</td>\n      <td>0.001777</td>\n      <td>0.007949</td>\n      <td>0.002870</td>\n      <td>0.011499</td>\n      <td>0.007262</td>\n      <td>0.002656</td>\n      <td>0.009778</td>\n      <td>...</td>\n      <td>0.006704</td>\n      <td>0.004441</td>\n      <td>0.008384</td>\n      <td>0.005722</td>\n      <td>0.003783</td>\n      <td>0.002551</td>\n      <td>0.007935</td>\n      <td>0.010838</td>\n      <td>0.014071</td>\n      <td>0.013151</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.140355</td>\n      <td>0.108687</td>\n      <td>0.065736</td>\n      <td>0.045820</td>\n      <td>0.056440</td>\n      <td>0.052694</td>\n      <td>0.067097</td>\n      <td>0.058526</td>\n      <td>0.059297</td>\n      <td>0.035693</td>\n      <td>...</td>\n      <td>0.032528</td>\n      <td>0.034391</td>\n      <td>0.040797</td>\n      <td>0.037143</td>\n      <td>0.034840</td>\n      <td>0.046042</td>\n      <td>0.066589</td>\n      <td>0.054724</td>\n      <td>0.057067</td>\n      <td>0.061895</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.163896</td>\n      <td>0.128998</td>\n      <td>0.084116</td>\n      <td>0.069041</td>\n      <td>0.074566</td>\n      <td>0.060650</td>\n      <td>0.080629</td>\n      <td>0.088747</td>\n      <td>0.107295</td>\n      <td>0.043712</td>\n      <td>...</td>\n      <td>0.054058</td>\n      <td>0.068965</td>\n      <td>0.050821</td>\n      <td>0.046897</td>\n      <td>0.045515</td>\n      <td>0.061525</td>\n      <td>0.078056</td>\n      <td>0.062134</td>\n      <td>0.069940</td>\n      <td>0.076313</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.194633</td>\n      <td>0.158656</td>\n      <td>0.115661</td>\n      <td>0.088772</td>\n      <td>0.096883</td>\n      <td>0.070891</td>\n      <td>0.092415</td>\n      <td>0.123537</td>\n      <td>0.156276</td>\n      <td>0.057363</td>\n      <td>...</td>\n      <td>0.111149</td>\n      <td>0.157310</td>\n      <td>0.064120</td>\n      <td>0.067974</td>\n      <td>0.070391</td>\n      <td>0.088147</td>\n      <td>0.088172</td>\n      <td>0.081652</td>\n      <td>0.083545</td>\n      <td>0.104001</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.271586</td>\n      <td>0.242121</td>\n      <td>0.192109</td>\n      <td>0.169416</td>\n      <td>0.160228</td>\n      <td>0.114997</td>\n      <td>0.146094</td>\n      <td>0.210620</td>\n      <td>0.263544</td>\n      <td>0.119883</td>\n      <td>...</td>\n      <td>0.166165</td>\n      <td>0.220927</td>\n      <td>0.115243</td>\n      <td>0.118714</td>\n      <td>0.149044</td>\n      <td>0.203736</td>\n      <td>0.121115</td>\n      <td>0.141722</td>\n      <td>0.183354</td>\n      <td>0.221810</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 21 columns</p>\n</div>"
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [],
   "source": [
    "# we will create a neural network with 2 hidden layers\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(21,)),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(48, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(24, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# use xgb regressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "model_2 = XGBRegressor(learning_rate=0.1, n_estimators=100, max_depth=3, min_child_weight=1, subsample=0.8, colsample_bytree=0.8, scale_pos_weight=1, seed=27)\n",
    "\n",
    "# use random forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_3 = RandomForestRegressor()\n",
    "\n",
    "# use linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model_4 = LinearRegression()\n",
    "\n",
    "# use a decision tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model_5 = DecisionTreeRegressor(min_samples_leaf=0.13, random_state=27) # max_depth=10, max_leaf_nodes=13\n",
    "# model_5.max_depth = 15\n",
    "# model_5.max_leaf_nodes = 24"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [],
   "source": [
    "# use test train split to split the data into training and validation data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = normalized_df\n",
    "y = df[\"symbol\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "172/172 [==============================] - 1s 3ms/step - loss: 11.3109 - mean_absolute_error: 11.3109 - mean_squared_error: 175.5115 - accuracy: 0.0475\n",
      "Epoch 2/10\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 11.3109 - mean_absolute_error: 11.3109 - mean_squared_error: 175.5116 - accuracy: 0.0475\n",
      "Epoch 3/10\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 11.3109 - mean_absolute_error: 11.3109 - mean_squared_error: 175.5115 - accuracy: 0.0475\n",
      "Epoch 4/10\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 11.3109 - mean_absolute_error: 11.3109 - mean_squared_error: 175.5115 - accuracy: 0.0475\n",
      "Epoch 5/10\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 11.3109 - mean_absolute_error: 11.3109 - mean_squared_error: 175.5116 - accuracy: 0.0475\n",
      "Epoch 6/10\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 11.3109 - mean_absolute_error: 11.3109 - mean_squared_error: 175.5115 - accuracy: 0.0475\n",
      "Epoch 7/10\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 11.3109 - mean_absolute_error: 11.3109 - mean_squared_error: 175.5116 - accuracy: 0.0475\n",
      "Epoch 8/10\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 11.3109 - mean_absolute_error: 11.3109 - mean_squared_error: 175.5116 - accuracy: 0.0475\n",
      "Epoch 9/10\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 11.3109 - mean_absolute_error: 11.3109 - mean_squared_error: 175.5116 - accuracy: 0.0475\n",
      "Epoch 10/10\n",
      "172/172 [==============================] - 0s 2ms/step - loss: 11.3109 - mean_absolute_error: 11.3109 - mean_squared_error: 175.5116 - accuracy: 0.0475\n"
     ]
    },
    {
     "data": {
      "text/plain": "DecisionTreeRegressor()",
      "text/html": "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor()</pre></div></div></div></div></div>"
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(\n",
    "    loss='mean_absolute_error',\n",
    "    optimizer=\"adam\",\n",
    "    metrics=['mean_absolute_error', 'mean_squared_error', 'accuracy']\n",
    ")\n",
    "\n",
    "# train the model\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    ")\n",
    "\n",
    "model_2.fit(\n",
    "    X_train, y_train\n",
    ")\n",
    "\n",
    "model_3.fit(\n",
    "    X_train, y_train\n",
    ")\n",
    "\n",
    "model_4.fit(\n",
    "    X_train, y_train\n",
    ")\n",
    "\n",
    "model_5.fit(\n",
    "    X_train, y_train\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 3ms/step - loss: 11.3768 - mean_absolute_error: 11.3768 - mean_squared_error: 176.9753 - accuracy: 0.0605\n"
     ]
    },
    {
     "data": {
      "text/plain": "[11.37677001953125,\n 11.37677001953125,\n 176.97531127929688,\n 0.060457516461610794]"
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the model\n",
    "model.evaluate(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9555818832427639"
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.score(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9879127343931685"
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.score(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5297807211967389"
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4.score(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9719673451337159"
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5.score(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 1.11442394e+01,  1.83313918e+00,  1.10215759e+01,  1.61080952e+01,\n        8.28927135e+00,  1.69721146e+01,  6.82213640e+00,  6.70756531e+00,\n        1.88213005e+01,  4.27743816e+00,  1.62996788e+01,  1.66688824e+01,\n        2.08596458e+01,  3.04592681e+00, -3.32274854e-01,  1.68301144e+01,\n        2.16422486e+00,  2.07066479e+01,  2.50376534e+00,  1.28763313e+01,\n        2.72872710e+00,  9.58587360e+00,  4.48008299e+00,  1.85841579e+01,\n        1.64537125e+01,  1.62246475e+01,  8.00806141e+00,  1.37960634e+01,\n        2.22720909e+00,  1.02160006e+01,  5.21272719e-01,  1.25571001e+00,\n        1.96516705e+01,  5.61368763e-01,  1.61822186e+01,  6.87061501e+00,\n        1.30778971e+01,  7.38046527e-01,  5.37457657e+00,  1.99029636e+01,\n        1.67310276e+01,  1.48475351e+01,  2.34255047e+01,  1.74456806e+01,\n        1.61951294e+01,  9.21380997e-01,  1.19463243e+01,  8.86633682e+00,\n        2.12764320e+01,  4.46290731e+00,  1.07946001e-01,  1.62496986e+01,\n        5.42125368e+00,  6.19128883e-01,  2.22312241e+01,  1.10763435e+01,\n        4.01607132e+00,  8.82114506e+00,  1.63734646e+01,  2.05473480e+01,\n        5.64727211e+00,  1.91259079e+01, -4.80158068e-02,  2.16365643e+01,\n        5.90595293e+00,  1.17299709e+01,  9.71925640e+00,  1.46529312e+01,\n        8.61919022e+00,  2.19302635e+01,  1.97523136e+01,  9.59952831e+00,\n        1.35744190e+01,  7.49745321e+00,  6.90214539e+00,  1.44761353e+01,\n        1.12920017e+01,  1.53583431e+01,  1.12465563e+01,  1.76918411e+01,\n        4.42378616e+00,  1.02847757e+01,  1.08475459e+00,  7.04994297e+00,\n        1.79745541e+01,  5.86272001e+00,  1.34225965e+00,  1.11911917e+01,\n        9.92142582e+00,  1.77977753e+01,  1.49967461e+01,  1.07482786e+01,\n        4.61216545e+00,  7.01848221e+00,  1.89947090e+01,  1.45740070e+01,\n        6.44101477e+00,  1.64389153e+01,  2.03717637e+00,  1.14296503e+01,\n        2.34051342e+01,  1.97780590e+01,  1.21057262e+01,  1.77773952e+01,\n        1.32902632e+01,  1.21833200e+01,  1.30561228e+01,  1.24532545e+00,\n        2.35056572e+01,  1.57249556e+01,  1.24937410e+01,  7.27778196e+00,\n        8.39710522e+00,  1.58568850e+01,  2.07157946e+00,  1.71943283e+01,\n        3.91574264e-01,  1.35818062e+01,  6.83634186e+00,  7.00632858e+00,\n        1.34225965e+00,  9.63586044e+00,  1.26917505e+01,  7.31201470e-01,\n        1.04953108e+01,  1.64389153e+01,  4.95707512e+00,  3.10612941e+00,\n        1.81411743e+01,  6.08407497e+00,  1.16765471e+01,  1.39904737e+01,\n        4.09407759e+00,  2.09076729e+01,  1.02708130e+01,  1.86138306e+01,\n        1.93096123e+01,  7.01848221e+00,  1.48604798e+00,  4.01031923e+00,\n        1.30068016e+01,  2.25173035e+01,  2.02062263e+01,  7.35701656e+00,\n        6.87206841e+00,  9.23021376e-01,  8.49743176e+00,  2.18515186e+01,\n        1.98030853e+01,  8.07768536e+00,  9.55111122e+00,  1.56603241e+01,\n        7.49643755e+00,  1.77541332e+01,  8.22052002e+00,  2.11712761e+01,\n        7.52723312e+00,  1.69632397e+01,  1.50252724e+01,  4.83562803e+00,\n        2.24382229e+01,  9.13684750e+00,  1.74852600e+01,  2.93566823e+00,\n        2.50376534e+00,  1.32134342e+01,  9.03707027e+00,  4.30601931e+00,\n        2.44369721e+00,  2.78456664e+00,  1.07260971e+01,  1.11321926e+01,\n        4.34931707e+00,  5.46162224e+00,  2.99284005e+00,  7.49949074e+00,\n        2.18640375e+00,  2.15577374e+01,  1.18947258e+01,  1.93953371e+00,\n        4.22978401e+00,  1.17218065e+01,  1.74195061e+01,  2.03709564e+01,\n        1.43876934e+01,  1.45123720e+01,  6.16232824e+00,  1.78051949e+01,\n        9.80208206e+00,  1.63622608e+01,  2.06148853e+01,  1.79397564e+01,\n        2.34343700e+01,  8.10874748e+00,  2.69613767e+00,  2.21368046e+01,\n        1.00707216e+01,  7.79525697e-01,  1.74468918e+01,  2.99865460e+00,\n        1.13169127e+01,  4.84589005e+00,  2.00368195e+01,  9.60009480e+00,\n        2.48427463e+00,  6.21671200e+00,  1.66711349e+01,  7.49123669e+00,\n        1.54589500e+01,  4.05261183e+00,  1.12936220e+01,  1.96879940e+01,\n        2.21359086e+00,  1.53758183e+01,  1.08584394e+01,  1.02499342e+01,\n        1.61470146e+01,  2.34987473e+00,  1.65000477e+01,  1.29923763e+01,\n        9.19633389e+00,  1.32134342e+01,  7.77725649e+00,  1.47754211e+01,\n        4.81046247e+00,  1.94719906e+01,  8.55192471e+00,  8.71623135e+00,\n        1.35439777e+01,  1.33693876e+01,  1.31947193e+01,  1.77997723e+01,\n        7.55418658e-01,  1.20063725e+01,  7.09845400e+00,  1.26172285e+01,\n        1.57782316e+01,  1.89604931e+01,  1.97714081e+01,  1.76756687e+01,\n        4.70601511e+00,  8.94734669e+00,  2.12656193e+01, -5.11799276e-01,\n        2.18640375e+00,  9.66646004e+00,  5.97132874e+00,  1.92624149e+01,\n        7.08671951e+00,  6.02284288e+00,  2.28219795e+01,  4.22978401e+00,\n        1.50650339e+01,  1.70579815e+01,  8.96352863e+00,  4.42698812e+00,\n        4.53734827e+00,  1.53414679e+01,  2.19177580e+00,  2.24677696e+01,\n        4.67849255e+00,  1.65205650e+01,  1.50552959e+01,  1.61462688e+01,\n        9.53903961e+00,  1.34902671e-02,  1.16633444e+01,  6.85543251e+00,\n        1.36350508e+01,  6.10360765e+00,  8.53324413e+00,  1.72885036e+01,\n        2.01042290e+01,  1.85493679e+01,  7.41085434e+00,  1.85894890e+01,\n        1.80951321e+00,  9.37031078e+00,  3.63216925e+00,  1.01628103e+01,\n        1.17916698e+01,  2.55175662e+00,  2.57385230e+00,  8.23021412e+00,\n        2.14551392e+01,  3.96033645e+00, -7.77312577e-01,  1.61444569e+01,\n        1.11180067e+01,  2.08876085e+00,  1.68224449e+01,  1.62009850e+01,\n        2.12764320e+01,  1.77832851e+01,  2.38441443e+00,  2.27303467e+01,\n        1.65878415e+00,  6.93731964e-01,  1.80283279e+01,  8.20481777e+00,\n        1.90980358e+01,  9.40597343e+00,  1.72682800e+01,  1.71126709e+01,\n        1.74203682e+01,  2.98268676e+00,  2.15624237e+01,  5.80864286e+00,\n        9.80567837e+00,  1.05675726e+01,  1.09630070e+01,  1.76858444e+01,\n        8.78630161e+00,  1.01507730e+01,  1.95721073e+01,  3.15372396e+00,\n        1.22609701e+01,  2.28979912e+01,  1.53810081e+01,  2.22232552e+01,\n        4.27048302e+00,  2.92813301e+00,  2.01515198e+01,  9.32495785e+00,\n        1.62725353e+01,  4.82221985e+00,  9.17069626e+00,  1.77230225e+01,\n        1.31068687e+01,  1.92127876e+01,  1.04971008e+01,  9.21107388e+00,\n        1.11023493e+01,  1.54331579e+01,  1.39868670e+01,  2.10164280e+01,\n        1.22099371e+01,  1.29901667e+01,  1.61849098e+01,  1.34548874e+01,\n        1.98269920e+01,  8.45633221e+00,  4.31310272e+00,  1.76675644e+01,\n        2.30954227e+01,  5.57617998e+00,  2.18640375e+00,  5.27117157e+00,\n        1.46670990e+01,  6.41829109e+00,  2.02747898e+01,  8.56733799e+00,\n        2.05779533e+01,  8.35437298e+00,  1.94801445e+01,  4.32100964e+00,\n        4.27841568e+00,  1.53655682e+01,  4.88772213e-01,  1.93953371e+00,\n        1.16919985e+01,  1.19333925e+01,  6.37778342e-01,  6.73173487e-01,\n        7.36096239e+00,  5.80298519e+00,  3.72512603e+00,  5.73469162e+00,\n        4.81199789e+00,  1.62366467e+01,  2.34315109e+01,  7.80365419e+00,\n        9.02976322e+00,  1.13846512e+01,  1.57901926e+01,  8.27257442e+00,\n        1.62402992e+01,  1.18667250e+01,  2.56902194e+00,  1.74903622e+01,\n        2.10562038e+01,  2.16422486e+00,  2.22114887e+01,  1.28203850e+01,\n        9.29629612e+00,  1.31695776e+01,  1.90581551e+01,  1.23841524e+01,\n        2.03905907e+01,  1.10184336e+01,  1.63917351e+01,  4.16600847e+00,\n        1.20190887e+01,  1.73012772e+01,  5.14412308e+00,  1.78386116e+01,\n        1.59316454e+01,  1.25232763e+01,  2.15471878e+01,  7.50283146e+00,\n        8.13815022e+00,  1.91949272e+01,  1.74727211e+01,  2.04409943e+01,\n        1.56578712e+01,  2.12327366e+01,  4.43970871e+00,  7.25314903e+00,\n        7.95189333e+00,  7.14593828e-01,  1.88407230e+01,  1.98083801e+01,\n        9.72150135e+00,  1.11091356e+01,  2.35324631e+01,  1.69776459e+01,\n        1.46113253e+01,  2.05120993e+00,  8.35551357e+00,  1.14502096e+01,\n        1.21209183e+01,  1.64680786e+01,  2.30954227e+01,  1.06392527e+01,\n        5.86685371e+00,  1.79803109e+00,  1.90301208e+01,  1.78814259e+01,\n        1.09151096e+01,  7.67024946e+00,  8.45692158e+00,  4.29556990e+00,\n        1.63303604e+01,  1.36516571e+01,  1.62813873e+01,  4.52897167e+00,\n        1.00890360e+01,  2.08137894e+01,  1.15315104e+01,  2.08297939e+01,\n        1.95352135e+01,  9.59149456e+00,  1.73756886e+01,  2.32783165e+01,\n        1.81728292e+00,  1.31354065e+01,  1.61002159e+01,  1.30392704e+01,\n        6.71295547e+00,  1.48604798e+00,  4.27926016e+00,  1.51533556e+01,\n        1.93738785e+01,  8.25175571e+00,  1.64317989e+01,  1.85549660e+01,\n        1.39567289e+01,  1.34443111e+01,  9.70808315e+00,  1.93096123e+01,\n        1.83443146e+01,  1.28152695e+01,  1.13534069e+01,  7.03430462e+00,\n        1.14319837e+00,  1.87319298e+01,  1.42596416e+01,  6.51877975e+00,\n        1.98342762e+01,  1.34668055e+01,  1.72530937e+01,  1.47305250e+01,\n        9.09874439e+00,  1.09364929e+01,  1.48467817e+01,  6.42772722e+00,\n        1.10402603e+01,  1.76871014e+01,  1.28524771e+01,  6.95551205e+00,\n        2.32323875e+01, -1.50597441e+00,  1.88086052e+01,  1.21087971e+01,\n        1.10028734e+01,  3.27129817e+00,  1.75465450e+01,  3.88177919e+00,\n        2.69102621e+00,  1.00862570e+01,  3.64534307e+00,  9.03804874e+00,\n        1.85743999e+01,  9.33546925e+00,  8.64040661e+00,  7.00949907e+00,\n        1.02488356e+01,  4.31503534e+00,  8.65964508e+00,  5.76757526e+00,\n        1.47878399e+01,  8.66433239e+00,  9.45828724e+00,  1.78355427e+01,\n        1.94675961e+01,  2.56902194e+00,  1.38736858e+01,  1.06450834e+01,\n        4.95707512e+00,  1.21975451e+01,  1.29128561e+01,  1.65600815e+01,\n        1.88668880e+01,  1.28878784e+01,  1.18671770e+01,  4.88310957e+00,\n        5.51129198e+00,  4.35926819e+00,  4.08826303e+00,  2.34535160e+01,\n        8.98091030e+00,  4.17663431e+00,  1.69776459e+01,  1.65099182e+01,\n        1.89906430e+00,  2.94427180e+00,  3.71872926e+00,  2.98268676e+00,\n        1.65021801e+01,  1.32134342e+01,  2.09116001e+01,  1.92198925e+01,\n        1.64803371e+01,  8.26393223e+00,  1.26122594e+00,  7.22842550e+00,\n        2.62673235e+00,  1.74852600e+01,  1.71524467e+01,  1.32415609e+01,\n        9.99064159e+00,  1.20197372e+01,  2.07838650e+01,  1.99523201e+01,\n        2.59752178e+00,  1.51500139e+01,  2.39604969e+01,  7.13908672e+00,\n        1.48604798e+00,  6.37392807e+00,  1.79949608e+01,  1.63650284e+01,\n        8.60471439e+00,  9.53841686e+00, -8.82218301e-01,  1.93325691e+01,\n        2.25403976e+00,  1.50802097e+01,  8.23666286e+00,  1.81502113e+01,\n        8.79450130e+00,  1.85386372e+01,  1.54458628e+01,  1.82414436e+00,\n        1.92246475e+01,  6.75060892e+00,  1.50559998e+01,  8.63734722e+00,\n        1.20211611e+01,  1.75477371e+01,  1.61080952e+01,  3.93483353e+00,\n        8.65242481e+00,  1.26022851e+00,  3.11447525e+00,  1.84230976e+01,\n        1.07391891e+01,  7.50009251e+00,  9.01741695e+00,  1.47816734e+01,\n        8.63878441e+00,  1.48604798e+00,  1.93747139e+01,  2.90310144e+00,\n        4.45747280e+00,  4.81696463e+00,  1.62900696e+01,  1.88166428e+01,\n        1.32671032e+01,  8.37954807e+00,  3.39180779e+00,  1.95733089e+01,\n        2.07812309e+01,  2.13615189e+01,  9.39229488e+00,  1.43790865e+01,\n        5.86545038e+00,  1.89621696e+01,  1.80434189e+01,  1.53015652e+01,\n        2.24925270e+01,  1.94867687e+01,  1.49812746e+01,  1.91613407e+01,\n        1.78500175e+01,  1.80397015e+01,  1.75750580e+01,  6.66925287e+00,\n        5.12044013e-01,  1.82025013e+01,  6.69704151e+00,  2.02482452e+01],\n      dtype=float32)"
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "outputs": [
    {
     "data": {
      "text/plain": "array([11.56,  1.  , 11.9 , 17.52, 10.  , 16.  ,  6.  ,  7.02, 20.  ,\n        4.  , 16.03, 16.97, 21.  ,  4.  ,  0.  , 18.  ,  2.  , 22.85,\n        2.03, 13.  ,  2.11,  9.93,  4.  , 17.03, 16.95, 16.11,  6.24,\n       13.1 ,  3.98,  8.82,  0.03,  0.03, 22.  ,  1.  , 15.91,  7.  ,\n       14.26,  0.  ,  3.12, 19.93, 15.13, 15.14, 23.  , 18.  , 21.79,\n        0.  , 14.64, 11.33, 22.85,  3.  ,  0.  , 17.91,  4.15,  0.  ,\n       22.  ,  5.42,  4.05,  8.  , 16.72, 23.  ,  6.  , 19.52,  0.13,\n       22.  ,  7.  , 11.01,  8.96, 12.63,  8.  , 23.  , 20.  , 11.47,\n       13.  ,  6.  ,  7.02, 17.32, 13.65, 16.38, 10.59, 16.09,  4.  ,\n        8.85,  1.  ,  7.  , 16.06,  6.  ,  1.  , 11.77,  9.95, 16.3 ,\n       12.21, 10.  ,  3.  ,  6.03, 19.01, 11.56,  5.9 , 16.79,  2.11,\n       12.91, 23.  , 20.  , 12.22, 17.03, 12.  , 12.  , 13.  ,  1.  ,\n       23.  , 16.95, 13.92,  7.  ,  9.  , 15.32,  2.  , 18.  ,  0.12,\n       14.03,  6.  ,  6.  ,  1.  , 12.73, 12.  ,  1.  ,  9.17, 17.  ,\n        5.  ,  4.  , 17.95,  6.17, 11.12,  9.25,  4.  , 22.  , 13.  ,\n       18.91, 19.  ,  6.06,  1.  ,  4.  , 13.  , 22.  , 20.93,  6.  ,\n        6.25,  1.  ,  8.16, 23.  , 19.99,  7.  , 12.44, 13.32,  7.12,\n       19.26,  8.  , 21.  ,  9.1 , 16.  , 15.  ,  5.  , 22.  ,  8.  ,\n       17.08,  4.  ,  2.14, 13.  ,  8.  ,  4.  ,  2.  ,  2.  , 13.  ,\n       12.98,  4.  ,  4.14,  3.  ,  7.  ,  2.  , 22.  , 11.95,  2.  ,\n        5.  ,  8.94, 17.9 , 22.  , 13.66, 14.87,  3.07, 16.95, 11.02,\n       17.  , 21.  , 19.42, 23.  ,  4.41,  3.42, 23.  ,  9.  ,  1.  ,\n       16.99,  3.  , 11.  ,  4.  , 22.  ,  8.  ,  2.  ,  6.  , 17.93,\n        6.  , 14.85,  3.  , 11.  , 19.46,  3.  , 15.  , 10.  , 10.  ,\n       16.35,  3.01, 16.12, 14.94,  9.  , 13.  ,  7.9 , 15.06,  3.  ,\n       19.03,  9.  ,  8.06, 15.  , 15.03, 15.  , 16.06,  1.  , 10.39,\n        7.  , 14.  , 16.36, 20.  , 19.91, 17.  ,  5.  ,  7.58, 21.  ,\n        0.05,  2.  ,  8.  ,  6.09, 19.68,  6.03,  4.07, 23.  ,  5.  ,\n       14.89, 16.47,  7.  ,  4.  ,  5.  , 14.99,  2.  , 23.  ,  5.  ,\n       16.77, 15.  , 16.36, 12.1 ,  0.  , 12.  ,  6.  , 14.  ,  5.  ,\n       10.  , 16.09, 19.94, 19.84,  7.  , 20.  ,  2.  , 10.81,  4.  ,\n        9.  ,  6.97,  1.27,  3.  ,  9.  , 21.  ,  4.07,  2.24, 10.39,\n       11.29,  2.  , 16.94, 16.93, 23.  , 17.02,  2.  , 22.24,  0.18,\n        1.  , 17.1 ,  6.69, 17.59,  8.  , 16.  , 17.  , 16.44,  3.  ,\n       23.  ,  7.24, 10.  , 12.02, 10.93, 17.94,  7.99,  8.  , 19.99,\n        4.  , 12.  , 22.61, 14.9 , 22.27,  4.  ,  3.  , 21.  ,  9.  ,\n       16.21,  5.  , 12.97, 18.53, 12.29, 19.2 , 13.  ,  8.  ,  9.97,\n       16.54, 14.14, 21.78, 13.55, 12.  , 15.44, 12.  , 20.  ,  6.12,\n        1.76, 17.99, 23.  ,  6.  ,  2.  ,  5.  , 15.  ,  8.98, 19.85,\n        9.  , 22.  ,  9.  , 19.38,  4.  ,  4.  , 17.82,  0.  ,  2.  ,\n       13.92, 11.99,  1.69,  0.  ,  6.02,  6.03,  5.07,  6.  ,  5.  ,\n       18.96, 23.  , 10.9 ,  9.  , 11.  , 16.69,  9.  , 16.07, 13.06,\n        2.  , 18.  , 21.  ,  2.  , 22.  , 13.  ,  8.51, 12.  , 20.  ,\n       12.  , 22.  , 13.94, 14.84,  3.  , 12.85, 16.04,  5.  , 16.18,\n       17.22, 14.16, 23.  ,  3.96,  8.  , 19.02, 18.  , 21.94, 16.15,\n       21.  ,  4.  ,  6.06,  6.08,  1.  , 19.08, 19.96,  9.79, 12.01,\n       23.  , 16.  , 14.09,  2.2 ,  8.06, 11.  , 11.  , 16.64, 22.91,\n       11.03,  6.  ,  0.56, 19.01, 18.  , 11.  ,  6.  ,  8.57,  3.  ,\n       16.06, 13.  , 16.94,  5.  , 10.  , 23.  , 12.78, 22.85, 19.03,\n       11.29, 20.95, 23.  ,  1.  , 15.12, 15.  , 13.  ,  6.  ,  1.  ,\n        3.04, 16.2 , 19.01, 10.13, 16.  , 19.04, 14.72, 14.22, 10.  ,\n       19.05, 21.  , 14.79, 12.96,  9.03,  0.  , 18.89, 14.69,  4.37,\n       19.02, 12.91, 16.28, 14.97, 10.  ,  8.85, 14.8 ,  6.  , 12.62,\n       16.95, 12.83,  6.  , 23.  ,  0.09, 18.94, 13.92, 11.  ,  4.08,\n       17.95,  4.  ,  4.02, 11.96,  1.36, 10.  , 19.06,  6.99,  9.18,\n        7.  ,  8.  ,  4.  ,  8.  ,  5.  , 15.  ,  7.  ,  9.  , 16.1 ,\n       21.  ,  2.  , 14.18, 10.  ,  5.  , 11.  , 14.  , 16.9 , 20.  ,\n       12.79, 12.  ,  3.96,  4.46,  3.  ,  4.  , 23.  , 10.02,  2.02,\n       16.  , 21.  ,  1.  ,  2.03,  4.  ,  3.  , 17.55, 13.  , 22.  ,\n       19.99, 16.96,  9.  ,  0.13,  7.  ,  2.07, 17.08, 16.18, 12.  ,\n       10.  , 12.85, 21.91, 19.94,  2.  , 15.  , 23.  ,  6.  ,  1.  ,\n        6.05, 18.86, 17.  ,  8.97,  8.98,  0.  , 19.98,  2.08, 15.02,\n        4.24, 20.74, 10.  , 17.14, 15.  ,  3.  , 18.99,  7.  , 14.94,\n        8.08, 12.  , 17.31, 17.46,  3.41,  8.94,  0.12,  3.  , 18.91,\n       11.  ,  6.1 ,  8.98, 14.98,  8.  ,  1.  , 18.87,  2.26,  3.  ,\n        5.  , 16.  , 19.84, 12.  ,  7.38,  2.  , 20.  , 21.  , 22.  ,\n        9.  , 15.11,  6.  , 19.14, 17.  , 15.  , 22.  , 19.63, 15.  ,\n       21.  , 16.1 , 17.33, 18.  ,  6.  ,  0.  , 16.24,  6.02, 21.  ])"
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "outputs": [
    {
     "data": {
      "text/plain": "array([13.07437122,  4.73866109,  4.83949967, 15.67763873,  7.48466572,\n       20.18938948, 14.60173463,  9.12839932, 19.71295832,  0.95299948,\n       22.39650952, 17.02730971, 10.63355084,  0.08010081,  5.65938937,\n       15.9432171 ,  5.72781055, 10.50545839,  7.9090825 , 11.59943918,\n        7.28407474,  8.64903212,  7.76709318, 15.45517457, 17.33119802,\n       17.24302258, 15.39855019, 20.98111086,  4.66020941, 15.21828012,\n        0.92275245,  2.30486946, 17.56547211,  3.60870732, 22.52701008,\n        8.74894023, 10.32594621,  4.7507306 ,  8.09775873, 19.92816884,\n       16.41226684, 13.42157508, 16.86255719, 15.25091864, 14.02837144,\n        5.36868254, 14.83468343,  9.47369491, 13.43560428, 12.2033514 ,\n        1.68493717, 14.69208269, 16.08298182,  2.05468284, 18.41342299,\n       12.17298427,  7.87432592, 11.23250716, 15.9523319 ,  8.92694343,\n        8.69744503, 15.80128402,  5.26131961, 14.13490187,  7.4321122 ,\n       12.03602799,  8.03243036, 16.64414112, 11.58080978, 15.64809569,\n       21.23134435,  3.84262725,  9.44581218, 12.09372395,  9.09706001,\n       16.86363617, 15.29347979, 12.78210183,  9.14657718, 21.33026221,\n        7.01109567, 10.08276236,  1.58154273,  7.75015582, 18.47259768,\n       10.62298228,  5.3830021 ,  9.84115128,  9.39686256, 18.95999142,\n       20.54693064,  9.58663342,  4.21471075, 10.77557327, 15.7000208 ,\n       20.45579637,  5.37695675, 22.88728032,  7.83408072, 10.28367408,\n       11.99813416, 20.08056686, 10.82033246, 15.5854713 , 14.20839943,\n        9.16104695,  8.39705993,  5.75154135, 17.92255963, 15.5503266 ,\n       12.54608509,  5.92733109, 12.74457888, 15.45030787,  8.22377869,\n       16.65347099,  2.00135422, 12.65777733,  5.36442041, 14.45800662,\n        6.50430373,  8.31698745, 19.08469888,  5.8937187 , 14.61326622,\n       22.60140189,  5.69498249,  3.2338965 , 15.8521004 ,  7.74398287,\n        9.40847221, 17.16583694,  3.9175371 , 17.44908711,  9.98704959,\n       20.3155512 , 16.13170243, 10.81282411,  5.16916378,  7.64515903,\n        8.88557969, 19.06758725, 12.18497714, 11.1051487 ,  5.53856198,\n        4.99894344,  6.88851463, 15.52393878, 19.38374257,  8.00608119,\n        8.77443028, 17.91102351,  8.22278607, 21.1021014 , 14.70450608,\n       13.95701956,  6.63419507, 22.23212935, 13.42332208,  6.07776012,\n       18.69794673, 12.10826576, 15.92421609,  4.55043504,  8.45937207,\n        9.08337048,  7.17823961,  6.72298483,  9.43209999,  2.6411776 ,\n        8.52482125,  7.40273337, -1.9242489 , -0.68644974,  6.62480944,\n        9.03143103,  7.82302371, 17.66350546, 10.38290829,  6.35139109,\n        7.90006183,  8.96790823, 15.69370783, 16.803724  , 17.26999686,\n       17.09416056,  6.5418096 , 17.57107122, 12.93791043, 14.04776203,\n        9.08919981, 21.60713074, 11.44012763, 12.51239022,  7.67930324,\n       14.86042932,  8.88738041,  4.41946193, 17.83896093,  5.81593795,\n       13.4372677 , 15.86813545, 17.1807109 , 14.8457663 ,  2.97979997,\n        6.91658011, 16.18666401, 14.54915196, 13.10561497,  9.71729375,\n        5.71462275, 14.50129235,  5.14344597, 15.38510811,  5.37365852,\n        9.60725014, 13.75168282,  6.31115603, 22.9080006 , 11.30572613,\n       12.01242132,  9.31602917,  4.01660289, 13.51248498,  5.87436218,\n       18.6528757 , 12.45353806,  3.57231811, 14.80432043, 12.17275413,\n        8.59912994, 20.01728816,  0.19157117, 12.08296087,  7.75727952,\n       12.17399265, 15.13852596, 17.68287472, 15.82936413, 15.49906341,\n        6.0853494 , 11.23674302, 11.25996951,  2.64768287,  8.34110689,\n       15.93148096,  9.1846142 , 14.8713389 , 10.83885547,  3.65915485,\n       13.70794616,  7.9163699 , 13.37920283, 20.72214622,  9.7540932 ,\n        3.96131706,  8.78214617, 10.64717488,  8.05336072, 10.94608434,\n        8.76868061, 24.46825051, 12.67251595, 17.52118134,  7.15078491,\n        6.98131556,  7.29813922,  9.23749413, 11.45322855,  7.49212807,\n        5.43179539, 18.27898881, 18.57799254, 14.64345446, 10.91262208,\n       13.65606615,  3.56199085,  4.83593511,  2.49373155,  8.54189722,\n       14.74038889,  1.93222751,  5.93131805, 13.10720779, 11.39124995,\n        9.73870694,  5.282571  , 12.83570789, 14.25494305,  8.02342843,\n       15.1643545 , 13.03401326, 13.64257039, 15.68275824,  6.54904197,\n       13.3206058 , 10.1423503 ,  2.81363832, 15.90939306,  6.65708906,\n       20.3843032 , 15.12503877, 21.26417323, 15.79717581, 21.48662722,\n        6.14426197, 13.93558479,  3.23922191, 10.74859543,  4.93511169,\n        5.31060579, 16.98855976,  9.53622836, 15.49075103, 20.12830975,\n       -1.83512758, 11.64731091, 11.73773799, 10.96862234, 12.85737534,\n        0.75788311,  6.57957835, 13.78509899, 10.00366287, 14.87090081,\n        6.40367244,  7.18010363, 16.03912144, 10.62557081, 21.14632543,\n        9.11723779,  7.67938241, 11.28679472, 13.92734388, 14.2807418 ,\n       20.58806649, 10.56814977, 18.81318032, 20.92233137, 19.13338414,\n       17.38392111,  8.88943211,  3.90288611, 18.950255  , 15.43250769,\n        9.9285331 ,  5.24926529,  7.12918833, 12.49596513,  5.86197219,\n       17.40091899, 12.94816818, 17.23535241,  6.89894485, 15.82246131,\n        6.84981525,  1.61467754, 16.67546128,  6.20751598,  6.64135594,\n       13.19345594,  9.54630137,  2.7530669 ,  2.86185083, 13.53693453,\n        8.76590992,  8.90250241, 10.17585356,  6.32154237, 16.66121509,\n       16.71919549, 11.50728067, 11.07111627, 14.91648953, 14.8611343 ,\n       12.23321248, 20.01358722,  7.78815504,  4.32392902, 14.84359833,\n        9.4730627 ,  8.14125243, 17.12091724,  8.97360691,  5.14127926,\n       12.84513219, 17.66124812, 10.57830073, 16.34729015, 13.59282048,\n       15.82834736, 12.39778341,  9.09791036, 19.57338384,  7.21055969,\n       20.05479678, 11.53093588,  9.84252825, 14.37321314, 15.52135088,\n        8.75737928, 26.36898803, 16.15168126, 16.50748132, 15.20618748,\n       11.75755386,  5.61267255, 11.49789762, 14.62728894,  4.70830385,\n       18.96963035, 17.80796107, 10.08086781,  7.15021305, 12.87754694,\n       21.33699558, 14.88733027, -1.62026993,  9.56844835,  9.50504902,\n       15.31080207, 15.85373835, 14.9946254 ,  1.27806649,  9.06682234,\n        2.56305886, 20.44048355, 16.11223081, 12.93518716, 13.56273495,\n       10.32588389,  9.83337843, 20.4775557 , 10.03599052, 17.17309489,\n        7.29144029,  8.37908309, 11.46288681,  9.59943006,  9.39410546,\n       16.36223528,  7.38480452,  6.52284132, 14.13921116,  4.93101471,\n       18.23381134, 16.6461825 , 11.34982549,  7.53235362,  3.33206571,\n        3.16862057, 14.82063092, 24.89494809, 11.75885137, 17.78703662,\n       16.31370701, 17.46238406,  9.63123592, 11.2191733 , 16.26336399,\n       11.52668944, 13.26235992,  9.16950145, 11.38166965,  6.0071568 ,\n       14.08680963, 15.65485137,  8.58990695, 15.61480256, 10.55924406,\n       19.36286864, 13.34902021, 10.07674594, 15.03320212, 13.50200039,\n       12.58067693, 21.17130075, 15.40155287, 10.95586855, 12.23221887,\n       16.0310712 ,  1.28150242, 22.11010352, 11.91148774, 14.6707697 ,\n        8.05861467, 15.9020064 ,  3.22779659,  6.60340298,  6.70377054,\n        9.94509771,  8.56162732, 14.49581519,  9.91729577,  8.89444454,\n        9.14535866,  7.85238398,  6.72828541, 14.41518617,  9.57487681,\n       12.19374291,  8.72526447,  9.20774036, 19.74856894, 14.28785773,\n        2.76759833, 10.81629757,  9.18746757,  5.94350296,  8.4879232 ,\n       11.97155639, 13.50405883, 19.86008242, 21.2420551 , 10.24430231,\n       10.82380801,  9.04483733,  9.67540858,  5.89179381, 16.75723345,\n        6.82509203,  0.27122307, 21.03231775, 11.32578195,  1.90254729,\n        8.120315  ,  3.54582922,  5.80734048, 18.80474051,  9.04424385,\n       18.40046375, 20.83720296, 15.99540695, 12.63604896,  6.01989946,\n       10.87845138,  8.87963615, 16.11615722, 18.77609785, 14.17580247,\n        9.00308214,  9.26724748, 16.2297979 , 15.2511693 ,  6.96992954,\n       13.4218813 , 15.91256929, 11.81624179,  5.05476298, 14.44886402,\n       17.41345277, 14.43552014, 11.32571831, 10.78295533,  4.68273457,\n       15.48578346,  6.65666213, 13.37469741,  5.63874165, 11.3708813 ,\n        6.19011986, 15.75831955, 15.08710861,  4.39826426, 18.72952479,\n        7.23086076, 16.41950852,  7.75783425, 11.28330601, 17.12011566,\n       15.24300308,  2.45311286, 13.83906521,  5.78799504,  9.1778678 ,\n       18.91100248, 10.80402037,  7.66241423, 12.60808329, 10.77984098,\n        8.13088942,  5.44107012, 25.79546259,  4.94561247,  9.82709114,\n        8.24760908, 20.22037693, 16.87593104, 14.28201393, 11.50033694,\n       12.3503696 , 18.62946388, 14.0675695 , 19.49870932, 13.30730758,\n       14.79005152,  6.27772508, 19.36950076, 14.51857899, 12.5535136 ,\n       18.57553565, 21.64456008, 14.04328603, 13.6891237 , 23.8476536 ,\n       12.77680069, 15.76896369,  6.36713443,  5.65571287, 18.95293341,\n       13.56009843, 11.15656956])"
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [
    {
     "data": {
      "text/plain": "array([11.,  1., 12., 18., 10., 16.,  6.,  7., 20.,  4., 16., 17., 21.,\n        4.,  0., 18.,  2., 23.,  2., 13.,  2., 10.,  4., 17., 17., 16.,\n        6., 11.,  3.,  8.,  0.,  0., 22.,  1., 16.,  7., 15.,  0.,  3.,\n       20., 18., 15., 23., 18., 23.,  0., 15., 13., 23.,  3.,  0., 18.,\n        4.,  0., 22.,  4.,  4.,  8., 17., 23.,  6., 20.,  0., 22.,  7.,\n       11.,  9., 17.,  8., 23., 20., 23., 13.,  6.,  7., 18., 19., 18.,\n       11., 16.,  4.,  9.,  1.,  7., 16.,  6.,  1., 12., 10., 16., 12.,\n       10.,  3.,  6., 19., 14.,  3., 17.,  2., 13., 23., 20., 12., 17.,\n       12., 12., 13.,  1., 23., 17., 14.,  7.,  9., 15.,  2., 18.,  0.,\n       15.,  6.,  6.,  1., 13., 12.,  1.,  8., 17.,  5.,  4., 19.,  4.,\n        6.,  7.,  4., 22., 13., 19., 19.,  6.,  1.,  4., 13., 22., 21.,\n        6.,  6.,  1.,  8., 23., 20.,  7., 13., 14.,  7., 19.,  8., 21.,\n        9., 16., 15.,  5., 22.,  8., 17.,  4.,  2., 13.,  8.,  4.,  2.,\n        2., 13., 13.,  4.,  4.,  3.,  7.,  2., 22., 12.,  2.,  5.,  9.,\n       18., 22., 15., 15.,  3., 17., 11., 17., 21., 20., 23.,  4.,  2.,\n       23.,  9.,  1., 17.,  3., 11.,  4., 22.,  8.,  2.,  6., 18.,  6.,\n       15.,  3., 11., 20.,  3., 15., 10., 10., 17.,  3., 16., 15.,  9.,\n       13.,  8., 15.,  3., 19.,  9.,  8., 15., 15., 15., 16.,  1., 11.,\n        7., 14., 17., 20., 20., 17.,  5.,  7., 21.,  0.,  2.,  8.,  2.,\n       20.,  6.,  4., 23.,  5., 15., 16.,  7.,  4.,  5., 15.,  2., 23.,\n        5., 17., 15., 16., 13.,  0., 12.,  6., 14.,  5., 10., 16., 20.,\n       20.,  7., 20.,  2., 11.,  4.,  9.,  4.,  1.,  3.,  9., 21.,  4.,\n        0., 11., 11.,  2., 17., 17., 23., 17.,  2., 23.,  0.,  1., 17.,\n        6., 16.,  8., 16., 17., 16.,  3., 23.,  4., 10., 12., 11., 18.,\n        8.,  8., 20.,  4., 12., 23., 15., 23.,  4.,  3., 21.,  9., 16.,\n        5., 13., 20., 12., 19., 13.,  8., 10., 17., 15., 22., 19., 12.,\n       15., 12., 20.,  6.,  1., 18., 23.,  6.,  2.,  5., 15.,  9., 20.,\n        9., 22.,  9., 20.,  4.,  4., 18.,  0.,  2., 14., 12.,  0.,  0.,\n        6.,  6.,  4.,  6.,  5., 19., 23., 14.,  9., 11., 17.,  9., 16.,\n       13.,  2., 18., 21.,  2., 22., 13.,  7., 12., 20., 12., 22., 14.,\n       17.,  3., 13., 16.,  5., 16., 18., 15., 23.,  2.,  8., 19., 18.,\n       22., 18., 21.,  4.,  6.,  6.,  1., 20., 20., 10., 12., 23., 16.,\n       14.,  1.,  7., 11., 11., 20., 23., 11.,  6.,  0., 19., 18., 11.,\n        6., 20.,  3., 16., 13., 17.,  5., 10., 23., 13., 23., 19., 13.,\n       21., 23.,  1., 15., 15., 13.,  6.,  1.,  3., 17., 19.,  9., 16.,\n       19., 17., 15., 10., 19., 21., 14., 13.,  9.,  0., 20., 15.,  4.,\n       19., 13., 19., 15., 10.,  8., 15.,  6., 14., 17., 13.,  6., 23.,\n        0., 19., 14., 11.,  4., 18.,  4.,  3., 12.,  0., 10., 19.,  7.,\n        9.,  7.,  8.,  4.,  8.,  5., 15.,  7.,  9., 16., 21.,  2., 14.,\n       10.,  5., 11., 14., 17., 20.,  4., 12.,  3.,  4.,  3.,  4., 23.,\n       10.,  1., 16., 21.,  1.,  2.,  4.,  3., 17., 13., 22., 20., 17.,\n        9.,  0.,  7.,  2., 17., 16., 12., 10., 13., 22., 20.,  2., 15.,\n       23.,  6.,  1.,  6., 19., 17.,  9.,  9.,  0., 20.,  2., 15.,  1.,\n       21., 10., 17., 15.,  3., 19.,  7., 15.,  8., 12., 20., 18.,  3.,\n        9.,  0.,  3., 19., 11.,  6.,  9., 15.,  8.,  1., 19.,  2.,  3.,\n        5., 16., 20., 12.,  7.,  2., 20., 21., 22.,  9., 15.,  6., 19.,\n       17., 15., 22., 20., 15., 21., 16., 18., 18.,  6.,  0., 16.,  6.,\n       21.])"
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "outputs": [
    {
     "data": {
      "text/plain": "112    11\n227     1\n97     12\n11     18\n77     10\n       ..\n109     6\n38      0\n164    16\n50      6\n201    21\nName: symbol, Length: 612, dtype: int64"
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [],
   "source": [
    "# save model 1\n",
    "#\n",
    "# pickle.dump(model_3, open(\"model_3.sav\", \"wb\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [],
   "source": [
    "# save model 5\n",
    "\n",
    "# pickle.dump(model_5, open(\"model_5.sav\", \"wb\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [],
   "source": [
    "# # load model\n",
    "# model = pickle.load(open(\"{{model_name}}.sav\", \"rb\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# we need to modify the data. Each row represents the xyz coordinates of several points on a hand\n",
    "# we will convert this into a series of distances between the points and the center of the hand\n",
    "test_data = pd.read_csv(\"testing_data_abcs.csv\")\n",
    "# test_data = pd.read_csv(\"training_data_0/C.csv\")\n",
    "test_data.drop(columns=[\"'symbol'\"], inplace=True)\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "# get the center of the hand\n",
    "for row in test_data.iterrows():\n",
    "    # get the xyz coordinates of the points\n",
    "    points = row[1][:].values.reshape(-1, 3)\n",
    "    # get the center of the hand\n",
    "    center = np.mean(points, axis=0)\n",
    "    # get the distances between the points and the center\n",
    "    distances = np.linalg.norm(points - center, axis=1)\n",
    "    # add the distances to the normalized dataframe\n",
    "    test_df = pd.concat([test_df, pd.DataFrame(distances).T])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.  , 0.  , 0.  , 0.  , 0.  , 0.27, 0.27, 0.66, 0.27, 0.  , 0.  ,\n       1.69, 1.81, 0.73, 1.46, 1.2 , 1.85, 2.82, 0.32, 1.63, 1.5 , 1.17,\n       1.5 , 1.52, 2.03, 2.99, 2.14, 2.88, 3.27, 3.42, 3.45, 0.31, 0.  ,\n       0.27, 5.07, 3.81, 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ,\n       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ,\n       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ,\n       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 2.28, 2.03, 1.99, 2.  , 1.99,\n       1.99, 2.  , 1.99, 2.  , 1.99, 1.99, 1.99, 1.99, 1.99, 1.99, 1.99,\n       2.  , 1.99, 2.  , 2.2 , 2.  , 2.11, 2.  , 3.72, 3.89, 4.64, 4.68,\n       3.3 , 4.26, 3.82, 3.39, 3.24, 3.03, 3.02, 3.02, 3.04, 3.04, 3.02,\n       3.04, 3.  , 3.  , 3.  , 3.  , 3.  , 3.  , 3.66, 3.52, 3.17, 3.  ,\n       3.31, 3.16, 3.07, 3.52, 3.07, 3.07, 3.41, 3.57, 3.68, 3.68, 4.07,\n       3.17, 3.49, 3.24, 3.24, 3.24, 3.43, 3.43, 3.19, 3.19, 3.19, 3.38,\n       3.17, 3.49, 3.03, 3.16, 3.17, 3.18, 3.16, 3.23, 3.22, 3.23, 3.22,\n       3.24, 3.06, 3.05, 3.04, 3.01, 3.  , 3.  , 3.01, 3.01, 3.01, 3.17,\n       3.16, 3.02, 2.02, 2.28, 2.31, 2.07, 2.36, 2.35, 2.33, 2.32, 2.28,\n       4.08, 4.12, 4.77, 4.9 , 4.72, 4.31, 4.52, 4.27, 4.97, 4.12, 4.86,\n       4.1 , 3.92, 4.51, 4.28, 4.16, 5.  , 4.55, 4.58, 4.22, 4.08, 4.29,\n       4.31, 4.31, 4.24, 4.57, 4.07, 4.29, 4.69, 4.31, 2.82, 3.37, 2.36,\n       2.36, 2.33, 4.3 , 3.94, 4.06, 4.16, 4.28, 4.1 , 3.99, 4.11, 3.9 ,\n       3.87, 4.08, 4.06, 3.95, 4.06, 4.06, 4.05, 3.93, 4.01, 3.98, 3.85,\n       3.98, 3.93, 3.93, 3.93, 3.94, 3.89, 3.98, 4.  , 4.  , 3.98, 3.81,\n       3.86, 3.89, 3.89, 4.06, 4.31, 4.62, 3.33, 4.89, 4.99, 4.99, 5.13,\n       5.09, 5.12, 5.  , 4.99, 4.99, 4.99, 4.96, 4.96, 5.  , 5.  , 5.04,\n       5.  , 5.  , 5.  , 4.91, 4.91, 4.91, 4.91, 4.91, 4.91, 4.91, 4.91,\n       4.91, 4.91, 4.91, 5.  , 4.91, 4.91, 4.91, 4.91, 4.91, 4.91, 4.91,\n       4.91, 4.92, 5.  , 5.  , 4.96, 4.96, 4.96, 4.96, 4.96, 4.96, 4.96,\n       4.93, 4.96, 4.96, 4.93, 4.93, 7.36, 8.08, 9.01, 8.42, 9.35, 9.23,\n       8.38, 8.82, 8.82, 8.69, 8.73, 9.25, 8.81, 9.43])"
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model_3.predict(test_df)\n",
    "predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "outputs": [],
   "source": [
    "# round the predictions\n",
    "predictions = np.round(predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "outputs": [],
   "source": [
    "# get the letter for each prediction\n",
    "letters = [num_to_letter[int(prediction)] for prediction in predictions]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A, B, A, C, B, C, D, A, C, B, C, D, C, D, A, F, E, B, C, E, F, D, E, D, E, D, E, D, E, D, C, E, F, E, F, E, F, E, F, E, F, E, F, E, F, E, F, E, D, C, E, F, D, F, H, I, K, I, K, I, K, "
     ]
    }
   ],
   "source": [
    "prev = None\n",
    "for letter in letters:\n",
    "    if prev is None:\n",
    "        print(letter, end=\", \")\n",
    "        prev = letter\n",
    "    else:\n",
    "        if prev != letter:\n",
    "            print(letter, end=\", \")\n",
    "            prev = letter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
